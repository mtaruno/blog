---
title: 'Building Software Engineering Agents with Large Language Models'
publishedAt: '2024-07-15'
summary: 'Insights from my experience developing software engineering agents on top of GLM models at Zhipu.AI.'
---

# Building Software Engineering Agents with Large Language Models


## The Challenge of Software Engineering Agents

Software engineering is a complex domain that requires not just understanding code, but also comprehending project structures, design patterns, and the intent behind implementations. 


Building an agent that can effectively assist in this domain presents several challenges:

1. **Context Understanding**: Software projects often span multiple files and directories with complex interdependencies. Agents need to understand the broader context beyond a single file.
2. **Tool Integration**: Effective software engineering agents need to interact with various development tools like version control systems, build tools, and IDEs.
3. **Code Generation Quality**: Generated code must not only be syntactically correct but also adhere to project-specific conventions and best practices.

## Our Approach

Our approach to building software engineering agents involved several key components:

### Repository-Level Understanding

We developed a pipeline that combined embedding techniques, retrieval algorithms, and chunking strategies to help the model understand code at the repository level. This allowed the agent to:
- Navigate complex codebases
- Understand relationships between different components
- Provide contextually relevant suggestions

### Benchmark Development

To evaluate and improve our agents, we curated a benchmark dataset consisting of over 250 entries from 10 open-source repositories. This benchmark helped us:

- Measure the accuracy of code retrieval
- Assess the quality of generated code
- Compare performance across different model versions

### In-Context Learning

We leveraged in-context learning to improve the agent's ability to adapt to different codebases and programming styles. By providing relevant examples from the codebase, we could guide the model to generate code that matched the project's conventions.

## Results and Insights

Our initial implementation achieved a baseline top-k retrieval accuracy of 35%, which provided a solid foundation for further improvements. Through iterative refinement, we learned several important lessons:

1. **Chunking Strategy Matters**: The way code is divided into chunks significantly impacts retrieval performance. Finding the right balance between chunk size and semantic coherence is crucial.



2. **User Experience is Key**: Technical performance metrics are important, but user experience considerations like response time, clarity of explanations, and the ability to refine requests are equally critical.
3. **Domain-Specific Fine-Tuning**: Models fine-tuned on programming tasks perform significantly better than general-purpose models, even when the latter have more parameters.

## Future Directions

The field of AI-assisted software engineering is rapidly evolving, and there are several exciting directions for future work:

1. **Multi-Modal Understanding**: Incorporating documentation, diagrams, and other non-code artifacts into the agent's understanding.
2. **Long-Term Memory**: Developing mechanisms for agents to remember past interactions and project-specific details over extended periods.
3. **Collaborative Workflows**: Creating agents that can effectively collaborate with human developers, understanding when to suggest solutions versus when to ask for clarification.

## Conclusion
Building software engineering agents presents unique challenges but also offers tremendous potential to enhance developer productivity. By focusing on repository-level understanding, rigorous benchmarking, and user experience, we can create AI assistants that truly augment human capabilities in software development.
The journey toward more capable software engineering agents is just beginning, and I'm excited to see how this field evolves in the coming years.
